#!/bin/bash
#SBATCH --account=project_465001640
#SBATCH --partition=dev-g
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --gpus-per-node=8
#SBATCH --mem=120G
#SBATCH --time=0-01:00:00           # Run time (d-hh:mm:ss)

# source ${HOME}/.bashrc
# SIF=~/images/rocm-vllm_ubuntu22.04_rocm6.3.1_py3.11_torch2.6.0_vllm_01-20-2025.sif # change the images path
# source /scratch/project_465001640/personal/dylan/venv/crux_env/bin/activate
# export SINGULARITY_BIND="/scratch/project_465001396,/scratch/project_465001640"
# singularity exec $SIF python3 vllm_launcher.py \
#     --model meta-llama/Llama-3.3-70B-Instruct \
#     --dtype bfloat16 --enforce-eager \
#     --tensor-parallel-size 4  --max-model-len 8192

module use /appl/local/csc/modulefiles/
module load pytorch/2.5

# Where to store the vLLM server log
VLLM_LOG=${SLURM_JOB_ID}.log

MODEL="meta-llama/Llama-3.3-70B-Instruct"
MODEL="meta-llama/Llama-3.1-8B-Instruct"

python -m vllm.entrypoints.openai.api_server \
       --model=$MODEL \
       --dtype bfloat16 \
       --tensor-parallel-size 8 \
       --pipeline-parallel-size 1 \
       --gpu-memory-utilization 0.9 \
       --max-model-len 8196 \
       --enforce-eager > $VLLM_LOG &
